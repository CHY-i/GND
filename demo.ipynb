{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b07c51e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from os.path import abspath, dirname, exists\n",
    "sys.path.append(abspath(dirname('demo')))\n",
    "from module import MADE, TraDE_binary, NADE,  Errormodel, mod2, Loading_code, read_code, Abstractcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "100c71cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(n_s, m, van, syndrome, device, dtype, k=1):\n",
    "    condition = syndrome*2-1\n",
    "    x = (van.partial_forward(n_s=n_s, condition=condition, device=device, dtype=dtype, k=k) + 1)/2\n",
    "    x = x[:, m:m+2*k]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdd763a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Code para'''\n",
    "c_type = 'rep'\n",
    "n, d, k, seed = 31 , 31, 1, 0\n",
    "info = read_code(d=d, k=k, n=n, seed=seed, c_type=c_type)\n",
    "Code = Loading_code(info)\n",
    "\n",
    "\n",
    "device = 'cuda:4'\n",
    "dtype = torch.float32\n",
    "m2 = mod2(device=device, dtype=dtype)\n",
    "\n",
    "'''Errormodel'''\n",
    "er = 0.001\n",
    "E = Errormodel(er, e_model='dep')\n",
    "\n",
    "'''Draw samples'''\n",
    "trials = 100000\n",
    "errors = E.generate_error(Code.n,  m=trials, seed=seed)\n",
    "syndrome = m2.commute(errors, Code.g_stabilizer)\n",
    "pe = E.pure(Code.pure_es, syndrome, device=device, dtype=dtype)\n",
    "logical_error = m2.commute(errors, Code.logical_opt)\n",
    "# print(logical_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ed16c9",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2de9156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.5141, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.4749, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.4621, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.4797, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.4653, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.4629, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.4928, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.4779, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.4875, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.4409, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "9999 LER: tensor(0.0296, device='cuda:4')\n",
      "Loss: tensor(0.4441, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.5015, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.4517, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.4543, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.4418, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.4249, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3612, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3484, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3192, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3231, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "19999 LER: tensor(0.0219, device='cuda:4')\n",
      "Loss: tensor(0.3236, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3595, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3361, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3405, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3316, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3280, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3525, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3094, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3684, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3581, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "29999 LER: tensor(0.0229, device='cuda:4')\n",
      "Loss: tensor(0.2998, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3190, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3406, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3214, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3060, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3303, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3140, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3225, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3146, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3291, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "39999 LER: tensor(0.0223, device='cuda:4')\n",
      "Loss: tensor(0.3047, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2972, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2817, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3207, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2787, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3045, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.3109, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2623, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2944, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2772, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "49999 LER: tensor(0.0222, device='cuda:4')\n",
      "Loss: tensor(0.2925, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2744, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2840, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2846, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2930, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2692, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2611, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2598, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2978, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2462, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "59999 LER: tensor(0.0210, device='cuda:4')\n",
      "Loss: tensor(0.2624, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2914, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2691, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2491, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2525, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2635, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2661, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2565, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2464, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2460, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "69999 LER: tensor(0.0229, device='cuda:4')\n",
      "Loss: tensor(0.2637, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2729, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2422, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2539, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2374, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2422, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2417, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2415, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2599, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2541, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "79999 LER: tensor(0.0216, device='cuda:4')\n",
      "Loss: tensor(0.2552, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2502, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2422, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2596, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2299, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2520, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2542, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2395, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2442, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2411, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "89999 LER: tensor(0.0226, device='cuda:4')\n",
      "Loss: tensor(0.2493, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2432, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2655, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2334, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2634, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2518, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2495, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2787, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2505, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "Loss: tensor(0.2528, device='cuda:4', dtype=torch.float32, grad_fn=<MeanBackward1>)\n",
      "99999 LER: tensor(0.0222, device='cuda:4')\n"
     ]
    }
   ],
   "source": [
    "'''net para'''\n",
    "epoch, batch, lr = 100000, 10000, 0.001\n",
    "ni = Code.m+2*k\n",
    "depth, width = 4, 30\n",
    "\n",
    "van = MADE(n=ni, depth=depth, width=width, residual=False).to(device).to(dtype)\n",
    "\n",
    "optimizer = torch.optim.Adam(van.parameters(), lr=lr)#, momentum=0.9)\n",
    "scheduler = StepLR(optimizer, step_size=2000, gamma=0.9)\n",
    "# loss_his = []\n",
    "# lo_his = []\n",
    "for l in range(epoch):\n",
    "    ers = E.generate_error(Code.n, m=batch, seed=False)\n",
    "\n",
    "    configs = torch.hstack([m2.commute(ers, Code.g_stabilizer), m2.commute(ers, Code.logical_opt)]).to(device).to(dtype)\n",
    "\n",
    "    # E.configs(sta=Code.g_stabilizer, log=Code.logical_opt, pe=pe, opts=ers).to(device).to(dtype)\n",
    "    # print(configs[10, Code.m:ni])\n",
    "    logp = van.log_prob((configs[:, :ni])*2-1) \n",
    "    loss = torch.mean((-logp), dim=0)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if optimizer.state_dict()['param_groups'][0]['lr'] > 0.0002 :\n",
    "        scheduler.step()\n",
    "    if (l+1)%1000==0:\n",
    "        print('Loss:', loss)\n",
    "    if (l+1) % 10000 == 0:\n",
    "        lconf = forward(n_s=trials, m=Code.m, van=van, syndrome=syndrome, device=device, dtype=dtype, k=k)\n",
    "        # print(lconf)\n",
    "\n",
    "        '''correction'''\n",
    "        # L = m2.confs_to_opt(confs=lconf, gs=Code.logical_opt)\n",
    "        # recover = m2.opt_prod(pe, L)\n",
    "        # # print(m2.commute(recover, Code.logical_opt))\n",
    "        # check = m2.opt_prod(recover, errors)\n",
    "        # commute = m2.commute(check, Code.logical_opt)\n",
    "        # # print(commute)\n",
    "        # fail = torch.count_nonzero(commute.sum(1))\n",
    "\n",
    "        fail = torch.count_nonzero((lconf-logical_error).sum(1))\n",
    "\n",
    "        logical_error_rate = fail/trials\n",
    "        print(l, 'LER:', logical_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3973f9cf",
   "metadata": {},
   "source": [
    "# Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6752d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020469999999999988\n"
     ]
    }
   ],
   "source": [
    "from pymatching import Matching\n",
    "\n",
    "def mwpm(Code, errors, er):\n",
    "    pcm = Code.PCM.cpu().numpy()\n",
    "    l1 = m2.rep(Code.logical_opt).int().cpu().numpy()\n",
    "    l = np.zeros_like(l1)\n",
    "    l[:, :n], l[:, n:] = l1[:, n:], l1[:, :n]\n",
    "\n",
    "    weights = np.ones(pcm.shape[1])*np.log((1-er)/er)\n",
    "    Decoder = Matching(pcm, weights=weights)\n",
    "    syndrome = m2.commute(errors, Code.g_stabilizer)\n",
    "    syndrome = syndrome.cpu().numpy()\n",
    "\n",
    "    error = m2.rep(errors).squeeze().int().cpu().numpy()\n",
    "    \n",
    "    correct_number = 0\n",
    "    t = 0\n",
    "    for j in range(trials):\n",
    "        e = error[j]\n",
    "        s = syndrome[j]\n",
    "\n",
    "        t1 = time.time()\n",
    "        #print(s)\n",
    "        recover = Decoder.decode(s)\n",
    "        check = (e + recover)%2\n",
    "        s = np.sum((check @ l.T) %2)\n",
    "        t2 = time.time()\n",
    "        t = t+(t2-t1)\n",
    "        if s == 0:\n",
    "            correct_number+=1\n",
    "        \n",
    "    lorate = 1 - correct_number/trials\n",
    "    return lorate\n",
    "\n",
    "lorate = mwpm(Code, errors, er)\n",
    "print(lorate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7940f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148fdbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [2., 2., 2.]])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from module import exact_config\n",
    "\n",
    "c_type = 'rep'\n",
    "n, d, k, seed = 3 , 3, 1, 0\n",
    "info = read_code(d=d, k=k, n=n, seed=seed, c_type=c_type)\n",
    "Code = Loading_code(info)\n",
    "\n",
    "device = 'cuda:4'\n",
    "dtype = torch.float64\n",
    "m2 = mod2(device=device, dtype=dtype)\n",
    "\n",
    "\n",
    "\n",
    "configs = exact_config(D=2, N=2**2)\n",
    "stas = m2.confs_to_opt(confs=configs, gs = Code.g_stabilizer)\n",
    "\n",
    "trials = 100\n",
    "E=Errormodel(e_rate=0.5, e_model='dep')\n",
    "errors = E.generate_error(n, trials)\n",
    "# print(errors)\n",
    "non_zero = 0\n",
    "for i in range(trials):\n",
    "    log_pcoset = torch.sum(E.log_probability(m2.opt_prod(errors[i], stas), device=device, dtype=dtype))\n",
    "    log_pcoset_z = torch.sum(E.log_probability(m2.opt_prod(m2.opt_prod(errors[i], Code.logical_opt[0]), stas), device=device, dtype=dtype))\n",
    "    if log_pcoset-log_pcoset_z !=0:\n",
    "        non_zero+=1\n",
    "print(non_zero)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
